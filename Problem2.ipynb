{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeremy Kazimer\n",
    "#### 5018-1732\n",
    "#### Assignment #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, our import statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main assignment\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as tr\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burger's Equation is a PDE and a particularly strange one at that.  Given by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "u_t + uu_x - \\kappa u_{xx} = 0 \n",
    "\\end{equation}\n",
    "\n",
    "where $u$ is the field of the system, $u_t$ is the first partial derivative with respect to $t$ (time), $u_x$ is the first partial derivative with respect to $x$ (position), $\\kappa$ is the diffusion constant, and $u_{xx}$ is the second partial derivative with respect to $x$.  \n",
    "\n",
    "This poses a problem from the first question of the assignment because it was rather easily able to be made discrete.  That is to say, by the form\n",
    "\\begin{equation}\n",
    "u_{i + 1} = u_i + u_i' \\delta t\n",
    "\\end{equation}\n",
    "\n",
    "However, since that's not the case, we can solve this numerically via a neural net from [this paper](https://arxiv.org/pdf/1711.10561.pdf) and its [respective GitHub page](https://github.com/maziarraissi/PINNs).  Note that all of the neural net code is from this repository, but I've stripped it of its more technical features so that it more accurately reflect the plain goal here.  It would be rather difficult to build our own neural net from the ground up, but understanding their code there has made it easier to work from the top down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a class from the neural network, which has been dubbed a 'Physics Informed Neural Network', or simply a neural network that's indicative of some physical process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    def __init__(self, x0, u0, layers, dt, q):\n",
    "        \n",
    "        '''\n",
    "            Initial conditions:\n",
    "            \n",
    "            lb -> the lower bound of the system.  We can set this arbitrarily to -1\n",
    "            ub -> the upper bound of the system.  We can set this arbitrarily to +1\n",
    "            x0 -> the starting x value.  Not arbitrary, but chosen at random\n",
    "            u0 -> the starting u value.  Not arbitrary, but chosen at random\n",
    "            layers -> the shape of each layer.\n",
    "            dt -> the time step.\n",
    "            q -> how many elements of the total data we take.\n",
    "        '''\n",
    "        \n",
    "        self.lb = -1\n",
    "        self.ub = 1\n",
    "        \n",
    "        self.x0 = x0\n",
    "        \n",
    "        self.u0 = u0\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.dt = dt\n",
    "        self.q = q\n",
    "        \n",
    "        '''\n",
    "            Intialization.  This creates the layers\n",
    "            for the weights and biases.  This is what allows\n",
    "            for the neural network to 'learn', so to speak.\n",
    "            Without them, we would not be able to really\n",
    "            make this work.  This works only after activation.\n",
    "        '''\n",
    "        \n",
    "        # Initialize NN\n",
    "        self.weights, self.biases = self.initialize_NN(layers)     \n",
    "        \n",
    "        '''\n",
    "            Placeholder variables.  We create these so we have them\n",
    "            to start with.  They're later modified and used\n",
    "            appropriately for gradients and the like.\n",
    "        '''\n",
    "        \n",
    "        # Placeholder variables\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        self.x0_tf = tf.placeholder(tf.float32, shape=(None, self.x0.shape[1]))\n",
    "        self.u0_tf = tf.placeholder(tf.float32, shape=(None, self.u0.shape[1]))\n",
    "        self.dummy_x0_tf = tf.placeholder(tf.float32, shape=(None, self.q))          \n",
    "        \n",
    "        '''\n",
    "            Here, we make a prediction for the base case\n",
    "            then take the difference between that and the\n",
    "            actual value so that we have a starting loss.\n",
    "            This allows for us to minimize!\n",
    "        '''\n",
    "        \n",
    "        self.U0_pred = self.net_U0(self.x0_tf) \n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.u0_tf - self.U0_pred))\n",
    "         \n",
    "        '''\n",
    "            This is what reduces loss and maximizes accuracy.\n",
    "            Consider that the method is the means by which loss is\n",
    "            minimized.  This is the same as the assignment from last semester.\n",
    "            \n",
    "            maxiter -> basically, break if you try to train more than this ammount.\n",
    "            maxfun -> basically, break if we need to use BFGS more than this amount.\n",
    "            ftol -> basically, break if our accuracy gets below this number, machine precision.\n",
    "            \n",
    "            The others I'm not sure about, so I can't say with confidence.\n",
    "        '''\n",
    "        \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 100,\n",
    "                                                                           'maxfun': 100,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})        \n",
    "        \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        '''\n",
    "            We create an initializer and then start the\n",
    "            session with this intializer so that\n",
    "            we can begin calculations on it.\n",
    "        '''\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        '''\n",
    "            Also storing this, for demonstration sake.\n",
    "        '''\n",
    "        \n",
    "        self.loss_array = []\n",
    "        self.time_array = []\n",
    "    \n",
    "    # Establish base layers for weights and biases.\n",
    "    def initialize_NN(self, layers):   \n",
    "        # Creating empty arrays to store\n",
    "        # the actual weights and biases\n",
    "        weights = []\n",
    "        biases = []\n",
    "        \n",
    "        num_layers = len(layers) \n",
    "        for l in range(0, num_layers - 1):\n",
    "            \n",
    "            # Here, we create weights with a truncated normal distribution.  \n",
    "            # So, each creation is random.  This allows us to handle the noise\n",
    "            # of the system, otherwise if everything was weighted 1 or 0 it wouldn't work.\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            \n",
    "            # We set the biases to start as zeros, but this can be updated as time progresses.\n",
    "            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            \n",
    "            weights.append(W)\n",
    "            biases.append(b)      \n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    # Generate random weights.\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        \n",
    "        # So our distribution exists around a certain\n",
    "        # average, but varies slightly.\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        \n",
    "        # This returns a random distribution of this size with mean = 0, std = xavier_stddev.\n",
    "        # So all values exist around 0, but fluctuate by this std.\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        # This just changes the spacing of X so that it's\n",
    "        # more uniform to our standard.\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        \n",
    "        # We iterate over all the layers, weights and biases included.\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            \n",
    "            # We have a 'tanh' activation.  It accumulates at each step.\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            \n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        \n",
    "        # We repeat for the last layers, to decide \n",
    "        # whether our data has 'activated' or not.\n",
    "        # In neuron speak, whether this neuron has\n",
    "        # fired.\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    # We calculate the gradients of a U0 vector.\n",
    "    def fwd_gradients_0(self, U, x):        \n",
    "        g = tf.gradients(U, x, grad_ys=self.dummy_x0_tf)[0]\n",
    "        return tf.gradients(g, self.dummy_x0_tf)[0]\n",
    "    \n",
    "    # This is just the equation as describe in the earlier part.\n",
    "    def net_U0(self, x):\n",
    "        U = self.neural_net(x, self.weights, self.biases)        \n",
    "        U_x = self.fwd_gradients_0(U, x)\n",
    "        U_xx = self.fwd_gradients_0(U_x, x)       \n",
    "        F = -1*U*U_x + 1*U_xx\n",
    "        U0 = U - self.dt*F\n",
    "        return U0\n",
    "    \n",
    "    # This is where we actually train the network to try and predict\n",
    "    # the field vectors! It does it for n times until it reaches the end.\n",
    "    def train(self, nIter):\n",
    "        \n",
    "        # Creating a dict of values to pass into training.\n",
    "        tf_dict = {self.x0_tf: self.x0, self.u0_tf: self.u0, \n",
    "                   self.dummy_x0_tf: np.ones((self.x0.shape[0], self.q))}\n",
    "        \n",
    "        \n",
    "        # Start iterating to train the network.\n",
    "        print('starting training...')\n",
    "        for it in range(nIter):\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Run the training.\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            \n",
    "            t1 = time.time()\n",
    "            # I'm storing loss and time taken here.\n",
    "            self.loss_array.append(self.sess.run(self.loss, tf_dict))\n",
    "            self.time_array.append(t1 - t0)\n",
    "            print('{:.2f}% done training!'.format(100*(it + 1)/nIter), '\\r', end = '')\n",
    "        \n",
    "        # We can reduce this loss further through optimization.\n",
    "        print('starting optimization...')\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss])\n",
    "    \n",
    "    # Make a simple prediction with base data.\n",
    "    def predict(self, x_star):\n",
    "        \n",
    "        U0_star = self.sess.run(self.U0_pred, {self.x0_tf: x_star, self.dummy_x0_tf: np.ones((x_star.shape[0], self.q))})        \n",
    "                    \n",
    "        return U0_star "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a lot of this code has been stripped away.  Before, there was the inclusion of constants from data files, a third partial derivative, and the like.  A lot of their functionality is designed to get the best result, but that's not really our goal, right? Our goal is to just numerically solve Burger's Equation with TensorFlow.  And so I did! The results from this can be seen below.  There are a lot of variables to keep track of, so I tried to describe them to the best of my ability with docstrings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pi/.local/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Variables: \n",
    "q -> some initial condition that I'm not fully sure of.\n",
    "skip -> the number of indices to skip.  Increasing it speeds up the function at the cost of accuracy\n",
    "data -> data from the original author.  We need this for their u values.\n",
    "N0 -> grid size?\n",
    "layers -> the size of each layer, assuming square.\n",
    "t_star -> time data.\n",
    "x_star -> grid data.\n",
    "u_star -> initial u data.\n",
    "idx_t -> index corresponding to starting time.\n",
    "idx_x -> index corresponding to starting position.\n",
    "x0 -> random starting point in space.\n",
    "u0 -> random starting point in time-space.\n",
    "dt -> time step\n",
    "model -> the PINN.\n",
    "U0_pred -> testing our model with a prediction.\n",
    "nIter -> number of training iterations.\n",
    "'''\n",
    "\n",
    "data = scipy.io.loadmat('burger_data.mat')\n",
    "\n",
    "skip = 100\n",
    "q = 50\n",
    "N0 = 20\n",
    "layers = [1, 50, 50, 50, 50, q]\n",
    "t_star = data['tt'].flatten()[:,None]\n",
    "x_star = data['x'].flatten()[:,None]\n",
    "u_star = np.real(data['uu'])\n",
    "idx_t = 40\n",
    "idx_x = np.random.choice(u_star.shape[0], N0, replace = False)\n",
    "x0 = x_star[idx_x,:]\n",
    "u0 = u_star[idx_x, idx_t][:,None]\n",
    "dt = np.float(t_star[idx_t + skip] - t_star[idx_t])        \n",
    "nIter = 150  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-df2c18b7f9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Building the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysicsInformedNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Testing our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2978549f9c2c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x0, u0, layers, dt, q)\u001b[0m\n\u001b[1;32m     73\u001b[0m         '''\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n\u001b[0m\u001b[1;32m     76\u001b[0m                                                                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'L-BFGS-B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                                                                 options = {'maxiter': 100,\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "model = PhysicsInformedNN(x0, u0, layers, dt, q)\n",
    "model.train(nIter = nIter)\n",
    "\n",
    "# Testing our model\n",
    "U0_pred = model.predict(x_star)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our code is accurate, despite the changes we made to the system.  You love to see it.  Regardless, we can plot a few other things.  Namely, important to the Pi Zero, the average runtimes for the most computationally expensive part of the run, the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# We skip the first element because that's like a 'start-up' time and is much greater than the rest.\n",
    "ax.hist(model.time_array[1:], 25, color = 'maroon');\n",
    "\n",
    "ax.set_ylabel('counts', fontsize = 14)\n",
    "ax.set_xlabel('runtime per iteration', fontsize = 14)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the overall runtime is pretty fast, to be fair.  At least, faster than expected.  We can see that this is vaguely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average runtime: t = 0.00256 pm 0.00004 seconds\n"
     ]
    }
   ],
   "source": [
    "print('average runtime: t = {:.5f} pm {:.5f} seconds'.format(np.mean(model.time_array[1:]), \n",
    "                                                             np.std(model.time_array[1:])/np.sqrt(len(model.time_array[1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the loss function over each iteration to show that perhaps their original amount was reaching the saturation point.  That is to say, the point by which iteration has nearly a zero reduction in loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# We skip the first element because that's like a 'start-up' time and is much greater than the rest.\n",
    "ax.plot(np.arange(nIter) + 1, model.loss_array, color = 'navy');\n",
    "\n",
    "ax.set_ylabel('loss', fontsize = 14)\n",
    "ax.set_xlabel('iteration', fontsize = 14)\n",
    "\n",
    "ax.set_xlim(1, nIter)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this just means perhaps we don't need as many iterations to cap out on our training model.  Of course, this doesn't translate to predicting on real data, which still has good loss, but it still shows that when optimizing something like this for the Pi Zero W that we can reduce the parameters as much as desired!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Conclusion_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that this assignment was mostly fair.  I would say that Problem 2 was the least fair and perhaps I only understand it somewhat thoroughly because I've experienced machine learning before.  Other than that, the big hassle here is just getting TensorFlow to run on the device, which will be a barrier for many in the future.  I can't even complain about the Pi Zero W here, since most of the limitations I've known for at least a week now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
